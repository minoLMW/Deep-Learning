{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnsJDsV98pfbifBkh+S+TY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. 자연어**\n","자연어는 인간이 일상적으로 의사소통에 사용하는 언어로, 말과 글을 통해 표현되는 언어를 말합니다. 자연어는 문법, 어휘, 맥락, 뉘앙스 등 복잡한 요소들로 이루어져 있어 규칙적인 구조와 함께 다양한 변형이 가능합니다. 컴퓨터 과학에서는 이러한 자연어를 이해하고 처리하기 위해 자연어 처리(NLP, Natural Language Processing) 기술이 사용되며, 이를 통해 텍스트 분석, 번역, 음성 인식, 챗봇과 같은 다양한 응용이 가능합니다. 자연어는 인간의 사고와 문화적 배경을 반영하므로, 이를 다루는 기술은 인문학적 이해와 기술적 접근이 결합되어야 합니다."],"metadata":{"id":"Scu59dtUZ28K"}},{"cell_type":"markdown","source":["# **2. 자연어 처리**\n","자연어 처리는(Natural Language Processing, NLP) 컴퓨터가 인간의 언어를 이해하고 처리하며 생성할 수 있도록 돕는 인공지능 기술 분야입니다. 이를 통해 텍스트나 음성 데이터를 분석하고, 번역, 요약, 감정 분석, 질의응답, 음성인식 등 다양한 작업을 수행할 수 있습니다. 자연어 처리는 언어학, 컴퓨터 과학, 인공지능의 융합으로 이루어지며, 형태소 분석, 구문 분석, 의미 분석 등 여러 단계를 포함합니다. 최근에는 딥러닝 기술과 대규모 언어 모델의 발전으로 자연어 처리 성능이 크게 향상되어 챗봇, 검색 엔진, 추천 시스템과 같은 실생활 응용에서 널리 사용되고 있습니다."],"metadata":{"id":"8e7_UPB5aa02"}},{"cell_type":"markdown","source":["# **3. 토큰화**\n","토큰화(Tokenization)는 텍스트 데이터를 분석하거나 처리하기 위해 문장을 의미 있는 단위로 나누는 과정입니다. 이 단위는 단어, 어절, 형태소, 또는 문자 단위일 수 있으며, 자연어 처리(NLP)의 기초 단계로 매우 중요한 역할을 합니다. 예를 들어, 영어에서는 공백이나 구두점을 기준으로 단어를 나누는 것이 일반적이지만, 한국어나 일본어처럼 공백이 명확하지 않은 언어에서는 형태소 분석기를 사용하여 어절이나 형태소 단위로 나누는 작업이 필요합니다."],"metadata":{"id":"yuAD6eDxad7o"}},{"cell_type":"markdown","source":["###문장 토큰화\n","문장 토큰화는 텍스트를 문장 단위로 나누는 과정입니다. 일반적으로 마침표(.), 물음표(?), 느낌표(!)와 같은 문장 부호를 기준으로 문장을 구분합니다. 예를 들어, \"안녕하세요. 오늘 날씨가 좋네요!\"라는 문장은 \"안녕하세요.\"와 \"오늘 날씨가 좋네요!\"로 나눌 수 있습니다. 하지만 약어(예: \"Dr.\", \"Mr.\")나 숫자(예: \"3.14\")처럼 마침표를 포함하지만 문장 경계가 아닌 경우를 처리해야 하므로 복잡한 규칙이나 모델이 필요합니다. 문장 토큰화는 문서 요약, 감정 분석, 번역 등의 작업에서 중요한 역할을 합니다. (한국어에서의 문장 토크나이저로 '[KSS](https://github.com/hyunwoongko/kss)'를 추천)"],"metadata":{"id":"DHi_yGq2agEy"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding\n","import numpy as np"],"metadata":{"id":"kIxnanJYah3T","executionInfo":{"status":"ok","timestamp":1755743357395,"user_tz":-540,"elapsed":6907,"user":{"displayName":"이민우","userId":"04570947474480009399"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 전처리할 텍스트를 정합니다.\n","text = '커피 한잔 어때'\n","\n","# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n","tokenizer =Tokenizer()\n","tokenizer.fit_on_texts([text])"],"metadata":{"id":"hbEzjdwydVY8","executionInfo":{"status":"ok","timestamp":1755743440715,"user_tz":-540,"elapsed":41,"user":{"displayName":"이민우","userId":"04570947474480009399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# texts_to_sequences로 텍스트를 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences([text])\n","sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkrIGw1Fds5B","executionInfo":{"status":"ok","timestamp":1755743506710,"user_tz":-540,"elapsed":12,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"9246b623-a765-4dd8-9fbb-e50d47ee7f98"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 2, 3]]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# 단어 인덱스 확인\n","word_index = tokenizer.word_index\n","word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7JUtwOkdt7s","executionInfo":{"status":"ok","timestamp":1755743585717,"user_tz":-540,"elapsed":18,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"1dee4024-e871-4045-9ad9-2cd64a5f477e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'커피': 1, '한잔': 2, '어때': 3}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["* Tokenizer는 기본적으로 띄어쓰기(whitespace)를 기준으로 단어를 나눕니다.\n","* 그리고 단어 안에 있는 일부 특수문자나 구두점(, . ! ? 등)은 제거합니다.\n","* 모두 소문자(lowercase)로 변환합니다 (영어 기준).\n","* 그래서 Tokenizer를 그냥 쓰면 공백 단위 토큰화 + 단순한 전처리만 적용됩니다."],"metadata":{"id":"3ZvwqOAheILc"}},{"cell_type":"code","source":["# 단어 빈도수 세기\n","\n","# 전처리하려는 세 개의 문장을 정합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n","       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n","       ]\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n","tokenizer = Tokenizer()            # 토큰화 함수 지정\n","tokenizer.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n","print(\"단어 카운트: \", tokenizer.word_counts)\n","\n","# 출력되는 순서는 랜덤입니다.\n","print(\"문장 카운트: \", tokenizer.document_count)\n","print(\"각 단어가 몇 개의 문장에 포함되어 있는가: \", tokenizer.word_docs)\n","print(\"각 단어에 매겨진 인덱스 값: \",  tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oo795TAae7RV","executionInfo":{"status":"ok","timestamp":1755743984107,"user_tz":-540,"elapsed":53,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"483611ff-6931-4242-999a-83c71e8690a8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 카운트:  OrderedDict({'먼저': 1, '텍스트의': 2, '각': 1, '단어를': 1, '나누어': 1, '토큰화합니다': 1, '단어로': 1, '토큰화해야': 1, '딥러닝에서': 2, '인식됩니다': 1, '토큰화한': 1, '결과는': 1, '사용할': 1, '수': 1, '있습니다': 1})\n","문장 카운트:  3\n","각 단어가 몇 개의 문장에 포함되어 있는가:  defaultdict(<class 'int'>, {'나누어': 1, '각': 1, '텍스트의': 2, '토큰화합니다': 1, '단어를': 1, '먼저': 1, '단어로': 1, '토큰화해야': 1, '인식됩니다': 1, '딥러닝에서': 2, '토큰화한': 1, '결과는': 1, '사용할': 1, '수': 1, '있습니다': 1})\n","각 단어에 매겨진 인덱스 값:  {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"]}]},{"cell_type":"code","source":["# 전처리할 텍스트를 정합니다.\n","texts = ['커피 한잔 어때', '오늘 날씨 참 좋네', '옷이 어울려요']\n","\n","# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","\n","# texts_to_sequences로 텍스트를 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","# 단어 인덱스 확인\n","word_index = tokenizer.word_index\n","print(\"\\n단어 인덱스:\\n\", word_index)\n","\n","# 패딩을 통해 시퀀스 길이를 맞춥니다.\n","print(\"\\n시퀀스:\\n\", sequences)\n","padded_sequences = pad_sequences(sequences, 4)\n","print(\"\\n패딩된 시퀀스:\\n\", padded_sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSvUqwZ9fanU","executionInfo":{"status":"ok","timestamp":1755744605256,"user_tz":-540,"elapsed":48,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"d34987e5-6519-4675-ee11-09961233af56"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 인덱스:\n"," {'커피': 1, '한잔': 2, '어때': 3, '오늘': 4, '날씨': 5, '참': 6, '좋네': 7, '옷이': 8, '어울려요': 9}\n","\n","시퀀스:\n"," [[1, 2, 3], [4, 5, 6, 7], [8, 9]]\n","\n","패딩된 시퀀스:\n"," [[0 1 2 3]\n"," [4 5 6 7]\n"," [0 0 8 9]]\n"]}]},{"cell_type":"code","source":["word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAc2K_hGiYka","executionInfo":{"status":"ok","timestamp":1755744672151,"user_tz":-540,"elapsed":12,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"c8c1ab00-d68c-43cf-d6e7-881b3f7992b5"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'커피': 1,\n"," '한잔': 2,\n"," '어때': 3,\n"," '오늘': 4,\n"," '날씨': 5,\n"," '참': 6,\n"," '좋네': 7,\n"," '옷이': 8,\n"," '어울려요': 9}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#input_dim에 1을 더하는 것은 인덱스 0을 패딩 값으로 사용하기 위함.\n","#Keras의 Tokenizer는 단어 인덱스를 1부터 시작하기 때문에, 인덱스 0은 패딩 값으로 예약\n","#output_dim은 단어가 임베딩될 벡터의 길이\n","model = Sequential()\n","model.add(Embedding(input_dim=len(word_index) + 1, output_dim=5))"],"metadata":{"id":"qTB-EXl0iJMx","executionInfo":{"status":"ok","timestamp":1755744714639,"user_tz":-540,"elapsed":10,"user":{"displayName":"이민우","userId":"04570947474480009399"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 임베딩 결과 확인\n","embedding_output = model.predict(padded_sequences)\n","embedding_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMRzhbxricC2","executionInfo":{"status":"ok","timestamp":1755744822780,"user_tz":-540,"elapsed":155,"user":{"displayName":"이민우","userId":"04570947474480009399"}},"outputId":"2c345ee2-e95a-44de-cf82-784cf5511bae"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[[ 0.02751099,  0.01008912, -0.03110201,  0.03604915,\n","          0.03560973],\n","        [ 0.0084965 , -0.01192682, -0.00015229, -0.0455676 ,\n","         -0.03364656],\n","        [-0.03685396, -0.0442373 ,  0.03992898, -0.04726202,\n","         -0.03492976],\n","        [-0.04002225, -0.01456455, -0.0097659 ,  0.03092232,\n","          0.01325655]],\n","\n","       [[ 0.01390633, -0.00760667, -0.01811299, -0.02103245,\n","          0.0471176 ],\n","        [ 0.02154091,  0.04201322, -0.04057438,  0.03816892,\n","          0.00402117],\n","        [-0.02684827, -0.02505111,  0.02228263,  0.01484526,\n","          0.02565173],\n","        [-0.026214  ,  0.00018742, -0.04132562, -0.0352936 ,\n","          0.04182719]],\n","\n","       [[ 0.02751099,  0.01008912, -0.03110201,  0.03604915,\n","          0.03560973],\n","        [ 0.02751099,  0.01008912, -0.03110201,  0.03604915,\n","          0.03560973],\n","        [-0.03484721,  0.02484593, -0.02369428, -0.0199361 ,\n","         -0.04246744],\n","        [-0.01037779,  0.04712305,  0.03959057,  0.04480605,\n","         -0.01209652]]], dtype=float32)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[],"metadata":{"id":"GoghT9Rwid8x"},"execution_count":null,"outputs":[]}]}